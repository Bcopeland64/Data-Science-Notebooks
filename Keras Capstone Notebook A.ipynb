{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "*First we import necessary libraries, datasets, and check for correctness of the data"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport numpy as np"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  Strength  \n0            1040.0           676.0   28     79.99  \n1            1055.0           676.0   28     61.89  \n2             932.0           594.0  270     40.27  \n3             932.0           594.0  365     41.05  \n4             978.4           825.5  360     44.30  "
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\nconcrete_data.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(1030, 9)"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data.shape"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>281.167864</td>\n      <td>73.895825</td>\n      <td>54.188350</td>\n      <td>181.567282</td>\n      <td>6.204660</td>\n      <td>972.918932</td>\n      <td>773.580485</td>\n      <td>45.662136</td>\n      <td>35.817961</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>104.506364</td>\n      <td>86.279342</td>\n      <td>63.997004</td>\n      <td>21.354219</td>\n      <td>5.973841</td>\n      <td>77.753954</td>\n      <td>80.175980</td>\n      <td>63.169912</td>\n      <td>16.705742</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>102.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>121.800000</td>\n      <td>0.000000</td>\n      <td>801.000000</td>\n      <td>594.000000</td>\n      <td>1.000000</td>\n      <td>2.330000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>192.375000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>164.900000</td>\n      <td>0.000000</td>\n      <td>932.000000</td>\n      <td>730.950000</td>\n      <td>7.000000</td>\n      <td>23.710000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>272.900000</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n      <td>185.000000</td>\n      <td>6.400000</td>\n      <td>968.000000</td>\n      <td>779.500000</td>\n      <td>28.000000</td>\n      <td>34.445000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>350.000000</td>\n      <td>142.950000</td>\n      <td>118.300000</td>\n      <td>192.000000</td>\n      <td>10.200000</td>\n      <td>1029.400000</td>\n      <td>824.000000</td>\n      <td>56.000000</td>\n      <td>46.135000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>540.000000</td>\n      <td>359.400000</td>\n      <td>200.100000</td>\n      <td>247.000000</td>\n      <td>32.200000</td>\n      <td>1145.000000</td>\n      <td>992.600000</td>\n      <td>365.000000</td>\n      <td>82.600000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\ncount  1030.000000         1030.000000  1030.000000  1030.000000   \nmean    281.167864           73.895825    54.188350   181.567282   \nstd     104.506364           86.279342    63.997004    21.354219   \nmin     102.000000            0.000000     0.000000   121.800000   \n25%     192.375000            0.000000     0.000000   164.900000   \n50%     272.900000           22.000000     0.000000   185.000000   \n75%     350.000000          142.950000   118.300000   192.000000   \nmax     540.000000          359.400000   200.100000   247.000000   \n\n       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\ncount       1030.000000       1030.000000     1030.000000  1030.000000   \nmean           6.204660        972.918932      773.580485    45.662136   \nstd            5.973841         77.753954       80.175980    63.169912   \nmin            0.000000        801.000000      594.000000     1.000000   \n25%            0.000000        932.000000      730.950000     7.000000   \n50%            6.400000        968.000000      779.500000    28.000000   \n75%           10.200000       1029.400000      824.000000    56.000000   \nmax           32.200000       1145.000000      992.600000   365.000000   \n\n          Strength  \ncount  1030.000000  \nmean     35.817961  \nstd      16.705742  \nmin       2.330000  \n25%      23.710000  \n50%      34.445000  \n75%      46.135000  \nmax      82.600000  "
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data.describe()"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "Cement                0\nBlast Furnace Slag    0\nFly Ash               0\nWater                 0\nSuperplasticizer      0\nCoarse Aggregate      0\nFine Aggregate        0\nAge                   0\nStrength              0\ndtype: int64"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "concrete_data.isnull().sum()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "*Split Predictors from Targets"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "concrete_data_columns = concrete_data.columns\n\npredictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\ntarget = concrete_data['Strength'] # Strength column"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1025</th>\n      <td>276.4</td>\n      <td>116.0</td>\n      <td>90.3</td>\n      <td>179.6</td>\n      <td>8.9</td>\n      <td>870.1</td>\n      <td>768.3</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1026</th>\n      <td>322.2</td>\n      <td>0.0</td>\n      <td>115.6</td>\n      <td>196.0</td>\n      <td>10.4</td>\n      <td>817.9</td>\n      <td>813.4</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1027</th>\n      <td>148.5</td>\n      <td>139.4</td>\n      <td>108.6</td>\n      <td>192.7</td>\n      <td>6.1</td>\n      <td>892.4</td>\n      <td>780.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1028</th>\n      <td>159.1</td>\n      <td>186.7</td>\n      <td>0.0</td>\n      <td>175.6</td>\n      <td>11.3</td>\n      <td>989.6</td>\n      <td>788.9</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1029</th>\n      <td>260.9</td>\n      <td>100.5</td>\n      <td>78.3</td>\n      <td>200.6</td>\n      <td>8.6</td>\n      <td>864.5</td>\n      <td>761.5</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "      Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n1025   276.4               116.0     90.3  179.6               8.9   \n1026   322.2                 0.0    115.6  196.0              10.4   \n1027   148.5               139.4    108.6  192.7               6.1   \n1028   159.1               186.7      0.0  175.6              11.3   \n1029   260.9               100.5     78.3  200.6               8.6   \n\n      Coarse Aggregate  Fine Aggregate  Age  \n1025             870.1           768.3   28  \n1026             817.9           813.4   28  \n1027             892.4           780.0   28  \n1028             989.6           788.9   28  \n1029             864.5           761.5   28  "
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "\npredictors.tail()"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0    79.99\n1    61.89\n2    40.27\n3    41.05\n4    44.30\nName: Strength, dtype: float64"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "target.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "n_cols = predictors_norm.shape[1] # number of predictors"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "*Importing Keras and developing baseline model"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": "# define regression model\ndef regression_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "scrolled": false
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/50\n - 1s - loss: 1649.2859 - val_loss: 1179.4343\nEpoch 2/50\n - 0s - loss: 1621.7120 - val_loss: 1158.3246\nEpoch 3/50\n - 0s - loss: 1588.2505 - val_loss: 1132.8195\nEpoch 4/50\n - 0s - loss: 1546.5845 - val_loss: 1101.3366\nEpoch 5/50\n - 0s - loss: 1494.3391 - val_loss: 1063.2558\nEpoch 6/50\n - 0s - loss: 1428.6567 - val_loss: 1016.5081\nEpoch 7/50\n - 1s - loss: 1349.3359 - val_loss: 959.7102\nEpoch 8/50\n - 1s - loss: 1252.5074 - val_loss: 894.4589\nEpoch 9/50\n - 0s - loss: 1144.3746 - val_loss: 820.4492\nEpoch 10/50\n - 0s - loss: 1025.0497 - val_loss: 741.1845\nEpoch 11/50\n - 0s - loss: 902.1086 - val_loss: 660.5489\nEpoch 12/50\n - 0s - loss: 782.2499 - val_loss: 580.8255\nEpoch 13/50\n - 0s - loss: 668.8661 - val_loss: 508.4601\nEpoch 14/50\n - 0s - loss: 569.8855 - val_loss: 441.6255\nEpoch 15/50\n - 0s - loss: 488.0572 - val_loss: 386.1966\nEpoch 16/50\n - 1s - loss: 422.5141 - val_loss: 338.9352\nEpoch 17/50\n - 0s - loss: 371.5298 - val_loss: 301.5338\nEpoch 18/50\n - 0s - loss: 333.7756 - val_loss: 272.1546\nEpoch 19/50\n - 0s - loss: 304.0815 - val_loss: 250.1895\nEpoch 20/50\n - 0s - loss: 281.9252 - val_loss: 234.1833\nEpoch 21/50\n - 0s - loss: 263.8417 - val_loss: 221.4650\nEpoch 22/50\n - 0s - loss: 249.4042 - val_loss: 212.0213\nEpoch 23/50\n - 0s - loss: 237.7540 - val_loss: 204.5574\nEpoch 24/50\n - 0s - loss: 227.9052 - val_loss: 199.5034\nEpoch 25/50\n - 0s - loss: 219.8449 - val_loss: 194.5001\nEpoch 26/50\n - 0s - loss: 213.0219 - val_loss: 191.3297\nEpoch 27/50\n - 0s - loss: 207.1158 - val_loss: 188.8583\nEpoch 28/50\n - 0s - loss: 201.9297 - val_loss: 186.4797\nEpoch 29/50\n - 1s - loss: 197.8294 - val_loss: 184.8136\nEpoch 30/50\n - 1s - loss: 193.5957 - val_loss: 183.0703\nEpoch 31/50\n - 0s - loss: 190.2093 - val_loss: 182.4014\nEpoch 32/50\n - 0s - loss: 187.1574 - val_loss: 180.6523\nEpoch 33/50\n - 0s - loss: 184.5395 - val_loss: 180.2984\nEpoch 34/50\n - 0s - loss: 181.7813 - val_loss: 179.1684\nEpoch 35/50\n - 0s - loss: 179.4452 - val_loss: 178.2881\nEpoch 36/50\n - 0s - loss: 177.1217 - val_loss: 177.1500\nEpoch 37/50\n - 0s - loss: 175.0225 - val_loss: 176.6058\nEpoch 38/50\n - 0s - loss: 173.2896 - val_loss: 175.5198\nEpoch 39/50\n - 0s - loss: 171.5456 - val_loss: 174.5011\nEpoch 40/50\n - 1s - loss: 170.0656 - val_loss: 173.0767\nEpoch 41/50\n - 0s - loss: 168.5265 - val_loss: 172.0299\nEpoch 42/50\n - 0s - loss: 167.4251 - val_loss: 172.0389\nEpoch 43/50\n - 0s - loss: 166.0020 - val_loss: 170.5414\nEpoch 44/50\n - 0s - loss: 164.5993 - val_loss: 169.6106\nEpoch 45/50\n - 0s - loss: 163.6321 - val_loss: 168.6435\nEpoch 46/50\n - 0s - loss: 162.5737 - val_loss: 167.3615\nEpoch 47/50\n - 0s - loss: 161.4997 - val_loss: 167.3299\nEpoch 48/50\n - 0s - loss: 160.5225 - val_loss: 166.1528\nEpoch 49/50\n - 0s - loss: 159.6679 - val_loss: 165.0425\nEpoch 50/50\n - 0s - loss: 158.6650 - val_loss: 163.2005\n"
                },
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f62600db390>"
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# build the model\nmodel = regression_model()\n# fit the model\nmodel.fit(predictors_norm, target, validation_split=0.3, epochs=50, verbose=2)"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/50\n721/721 [==============================] - 1s 941us/step - loss: 94.7678 - val_loss: 139.4180\nEpoch 2/50\n721/721 [==============================] - 1s 704us/step - loss: 94.4359 - val_loss: 135.8589\nEpoch 3/50\n721/721 [==============================] - 0s 683us/step - loss: 93.9662 - val_loss: 134.7360\nEpoch 4/50\n721/721 [==============================] - 0s 668us/step - loss: 93.5664 - val_loss: 137.6809\nEpoch 5/50\n721/721 [==============================] - 1s 694us/step - loss: 92.9880 - val_loss: 135.0897\nEpoch 6/50\n721/721 [==============================] - 1s 695us/step - loss: 92.5457 - val_loss: 134.7120\nEpoch 7/50\n721/721 [==============================] - 0s 692us/step - loss: 92.2759 - val_loss: 134.7925\nEpoch 8/50\n721/721 [==============================] - 0s 668us/step - loss: 91.6211 - val_loss: 133.9269\nEpoch 9/50\n721/721 [==============================] - 0s 662us/step - loss: 91.4686 - val_loss: 134.3265\nEpoch 10/50\n721/721 [==============================] - 0s 666us/step - loss: 90.9530 - val_loss: 132.5238\nEpoch 11/50\n721/721 [==============================] - 1s 729us/step - loss: 90.5266 - val_loss: 134.8821\nEpoch 12/50\n721/721 [==============================] - 1s 710us/step - loss: 90.0087 - val_loss: 132.4405\nEpoch 13/50\n721/721 [==============================] - 1s 776us/step - loss: 89.7942 - val_loss: 132.3441\nEpoch 14/50\n721/721 [==============================] - 0s 691us/step - loss: 89.4537 - val_loss: 131.3445\nEpoch 15/50\n721/721 [==============================] - 0s 690us/step - loss: 88.9975 - val_loss: 133.7250\nEpoch 16/50\n721/721 [==============================] - 0s 692us/step - loss: 88.4868 - val_loss: 132.8983\nEpoch 17/50\n721/721 [==============================] - 0s 693us/step - loss: 88.1278 - val_loss: 133.8475\nEpoch 18/50\n721/721 [==============================] - 1s 696us/step - loss: 87.7153 - val_loss: 131.0742\nEpoch 19/50\n721/721 [==============================] - 1s 720us/step - loss: 87.3649 - val_loss: 132.7042\nEpoch 20/50\n721/721 [==============================] - 1s 1ms/step - loss: 86.9007 - val_loss: 130.8013\nEpoch 21/50\n721/721 [==============================] - 1s 1ms/step - loss: 86.6448 - val_loss: 130.8237\nEpoch 22/50\n721/721 [==============================] - 0s 668us/step - loss: 86.1627 - val_loss: 132.6786\nEpoch 23/50\n721/721 [==============================] - 1s 728us/step - loss: 85.8938 - val_loss: 131.8005\nEpoch 24/50\n721/721 [==============================] - 1s 716us/step - loss: 85.2870 - val_loss: 131.1726\nEpoch 25/50\n721/721 [==============================] - 1s 721us/step - loss: 84.8101 - val_loss: 131.7691\nEpoch 26/50\n721/721 [==============================] - 0s 693us/step - loss: 84.6650 - val_loss: 133.8882\nEpoch 27/50\n721/721 [==============================] - 0s 689us/step - loss: 84.2891 - val_loss: 133.1792\nEpoch 28/50\n721/721 [==============================] - 1s 703us/step - loss: 83.7005 - val_loss: 134.3176\nEpoch 29/50\n721/721 [==============================] - 1s 708us/step - loss: 83.4822 - val_loss: 132.5390\nEpoch 30/50\n721/721 [==============================] - 1s 695us/step - loss: 82.9538 - val_loss: 135.3006\nEpoch 31/50\n721/721 [==============================] - 0s 693us/step - loss: 82.5489 - val_loss: 134.6721\nEpoch 32/50\n721/721 [==============================] - 1s 735us/step - loss: 82.0708 - val_loss: 134.4123\nEpoch 33/50\n721/721 [==============================] - 1s 696us/step - loss: 81.7765 - val_loss: 134.9494\nEpoch 34/50\n721/721 [==============================] - 1s 718us/step - loss: 81.4673 - val_loss: 134.4672\nEpoch 35/50\n721/721 [==============================] - 0s 693us/step - loss: 81.2729 - val_loss: 134.0515\nEpoch 36/50\n721/721 [==============================] - 1s 725us/step - loss: 80.7436 - val_loss: 134.7488\nEpoch 37/50\n721/721 [==============================] - 1s 1ms/step - loss: 80.4717 - val_loss: 133.3321\nEpoch 38/50\n721/721 [==============================] - 0s 667us/step - loss: 80.1204 - val_loss: 134.4499\nEpoch 39/50\n721/721 [==============================] - 1s 696us/step - loss: 79.7687 - val_loss: 135.1515\nEpoch 40/50\n721/721 [==============================] - 1s 1ms/step - loss: 79.3128 - val_loss: 135.2179\nEpoch 41/50\n721/721 [==============================] - 1s 1ms/step - loss: 79.0223 - val_loss: 133.9051\nEpoch 42/50\n721/721 [==============================] - 0s 685us/step - loss: 78.5111 - val_loss: 135.7084\nEpoch 43/50\n721/721 [==============================] - 0s 692us/step - loss: 78.1634 - val_loss: 135.1774\nEpoch 44/50\n721/721 [==============================] - 0s 675us/step - loss: 77.7561 - val_loss: 136.1186\nEpoch 45/50\n721/721 [==============================] - 0s 668us/step - loss: 77.0224 - val_loss: 135.8704\nEpoch 46/50\n721/721 [==============================] - 1s 721us/step - loss: 76.8020 - val_loss: 135.7804\nEpoch 47/50\n721/721 [==============================] - 0s 693us/step - loss: 76.0575 - val_loss: 138.1361\nEpoch 48/50\n721/721 [==============================] - 0s 662us/step - loss: 75.5589 - val_loss: 134.6390\nEpoch 49/50\n721/721 [==============================] - 0s 665us/step - loss: 74.5073 - val_loss: 138.2985\nEpoch 50/50\n721/721 [==============================] - 0s 693us/step - loss: 73.3679 - val_loss: 140.7836\n"
                },
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f62b810eef0>"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# fit the model\nmodel.fit(predictors_norm, target, validation_split=0.3, epochs=50, verbose=1)"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f6260150358>"
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# fit the model\nmodel.fit(predictors_norm, target, validation_split=0.3, epochs=50, verbose=0)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "*Normalized Data"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "predictors_norm = (predictors - predictors.mean()) / predictors.std()"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.476712</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>-0.916319</td>\n      <td>-0.620147</td>\n      <td>0.862735</td>\n      <td>-1.217079</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.476712</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>-0.916319</td>\n      <td>-0.620147</td>\n      <td>1.055651</td>\n      <td>-1.217079</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.491187</td>\n      <td>0.795140</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>3.551340</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.491187</td>\n      <td>0.795140</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>5.055221</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.790075</td>\n      <td>0.678079</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.070492</td>\n      <td>0.647569</td>\n      <td>4.976069</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.145138</td>\n      <td>0.464818</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-1.291914</td>\n      <td>0.701883</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.945704</td>\n      <td>0.244603</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>5.055221</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.945704</td>\n      <td>0.244603</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-0.145138</td>\n      <td>0.464818</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-1.291914</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.854740</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.790075</td>\n      <td>0.678079</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.070492</td>\n      <td>0.647569</td>\n      <td>0.701883</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-0.790075</td>\n      <td>0.678079</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.070492</td>\n      <td>0.647569</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.400222</td>\n      <td>-0.305934</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>3.551340</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-0.872367</td>\n      <td>1.345678</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-1.291914</td>\n      <td>0.701883</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.218476</td>\n      <td>0.024388</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-1.291914</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.945704</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-1.291914</td>\n      <td>0.701883</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-1.354634</td>\n      <td>1.570529</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.952763</td>\n      <td>0.415580</td>\n      <td>0.701883</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.582090</td>\n      <td>-0.416042</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-1.291914</td>\n      <td>5.055221</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.945704</td>\n      <td>0.244603</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>0.701883</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.854740</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>2.126612</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1.400222</td>\n      <td>-0.305934</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>2.126612</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>-1.354634</td>\n      <td>1.570529</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.952763</td>\n      <td>0.415580</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>-1.354634</td>\n      <td>1.570529</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.952763</td>\n      <td>0.415580</td>\n      <td>-0.675355</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>-1.354634</td>\n      <td>1.570529</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.952763</td>\n      <td>0.415580</td>\n      <td>2.126612</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.945704</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-1.291914</td>\n      <td>5.055221</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.945704</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-1.291914</td>\n      <td>3.551340</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.945704</td>\n      <td>0.244603</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>3.551340</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.582090</td>\n      <td>-0.416042</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-1.291914</td>\n      <td>2.126612</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1.400222</td>\n      <td>-0.305934</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1.854740</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>-0.612034</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>-1.332626</td>\n      <td>1.074465</td>\n      <td>1.179925</td>\n      <td>-0.377784</td>\n      <td>0.785983</td>\n      <td>-1.161599</td>\n      <td>0.146172</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1001</th>\n      <td>0.159150</td>\n      <td>0.733712</td>\n      <td>0.823658</td>\n      <td>0.924067</td>\n      <td>-0.034259</td>\n      <td>-1.215616</td>\n      <td>-1.475261</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1002</th>\n      <td>0.384016</td>\n      <td>1.046649</td>\n      <td>-0.846733</td>\n      <td>0.418312</td>\n      <td>-0.268614</td>\n      <td>-1.323649</td>\n      <td>0.005232</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1003</th>\n      <td>0.811741</td>\n      <td>1.310907</td>\n      <td>-0.846733</td>\n      <td>0.455775</td>\n      <td>0.066178</td>\n      <td>-1.911400</td>\n      <td>-0.208048</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1004</th>\n      <td>-0.013089</td>\n      <td>0.637513</td>\n      <td>0.722091</td>\n      <td>-0.429296</td>\n      <td>0.551628</td>\n      <td>-1.901111</td>\n      <td>0.390635</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1005</th>\n      <td>-0.278144</td>\n      <td>0.268942</td>\n      <td>0.334573</td>\n      <td>0.572848</td>\n      <td>0.350753</td>\n      <td>-1.767356</td>\n      <td>0.596432</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1006</th>\n      <td>-1.115414</td>\n      <td>-0.856472</td>\n      <td>1.503377</td>\n      <td>0.001532</td>\n      <td>0.919901</td>\n      <td>0.647955</td>\n      <td>-0.557280</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1007</th>\n      <td>-1.201533</td>\n      <td>1.965756</td>\n      <td>-0.846733</td>\n      <td>-0.059346</td>\n      <td>0.752504</td>\n      <td>0.631236</td>\n      <td>-0.946424</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1008</th>\n      <td>-1.157517</td>\n      <td>1.322497</td>\n      <td>1.440874</td>\n      <td>1.013042</td>\n      <td>0.852942</td>\n      <td>-1.854812</td>\n      <td>-0.796753</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1009</th>\n      <td>0.162020</td>\n      <td>-0.856472</td>\n      <td>0.825221</td>\n      <td>0.226312</td>\n      <td>-0.017520</td>\n      <td>-1.207899</td>\n      <td>0.519102</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1010</th>\n      <td>0.351482</td>\n      <td>-0.856472</td>\n      <td>1.129922</td>\n      <td>1.317431</td>\n      <td>-0.084478</td>\n      <td>-1.445829</td>\n      <td>-0.461241</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1011</th>\n      <td>0.058677</td>\n      <td>0.540155</td>\n      <td>0.620524</td>\n      <td>0.282507</td>\n      <td>0.501409</td>\n      <td>-0.881228</td>\n      <td>-0.968875</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1012</th>\n      <td>0.425162</td>\n      <td>1.072147</td>\n      <td>-0.846733</td>\n      <td>-0.354369</td>\n      <td>0.451190</td>\n      <td>-1.174460</td>\n      <td>0.204793</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1013</th>\n      <td>0.715097</td>\n      <td>-0.856472</td>\n      <td>1.365871</td>\n      <td>0.549433</td>\n      <td>0.802723</td>\n      <td>-2.205919</td>\n      <td>0.060112</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1014</th>\n      <td>-1.427357</td>\n      <td>1.536917</td>\n      <td>1.667448</td>\n      <td>-0.124907</td>\n      <td>-0.117958</td>\n      <td>-1.363518</td>\n      <td>-0.473714</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1015</th>\n      <td>0.395499</td>\n      <td>0.865841</td>\n      <td>-0.846733</td>\n      <td>0.198215</td>\n      <td>0.384232</td>\n      <td>-0.281901</td>\n      <td>-0.799248</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1016</th>\n      <td>-1.119242</td>\n      <td>-0.856472</td>\n      <td>2.279976</td>\n      <td>-0.017199</td>\n      <td>1.070557</td>\n      <td>-1.589873</td>\n      <td>0.903257</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1017</th>\n      <td>0.312250</td>\n      <td>-0.856472</td>\n      <td>0.912725</td>\n      <td>-0.546369</td>\n      <td>0.652066</td>\n      <td>-0.612431</td>\n      <td>0.116238</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1018</th>\n      <td>0.384973</td>\n      <td>-0.856472</td>\n      <td>1.151798</td>\n      <td>0.043678</td>\n      <td>0.886421</td>\n      <td>-1.322363</td>\n      <td>0.076326</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1019</th>\n      <td>-1.353677</td>\n      <td>1.043172</td>\n      <td>1.148673</td>\n      <td>2.581819</td>\n      <td>-0.067739</td>\n      <td>-1.341654</td>\n      <td>-1.471519</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1020</th>\n      <td>0.069203</td>\n      <td>0.545950</td>\n      <td>-0.846733</td>\n      <td>-0.195150</td>\n      <td>0.133137</td>\n      <td>-0.836214</td>\n      <td>0.697460</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1021</th>\n      <td>0.162977</td>\n      <td>-0.856472</td>\n      <td>0.825221</td>\n      <td>1.317431</td>\n      <td>0.819463</td>\n      <td>-1.200182</td>\n      <td>-0.366450</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1022</th>\n      <td>-0.159491</td>\n      <td>0.430047</td>\n      <td>0.504893</td>\n      <td>0.652457</td>\n      <td>-0.050999</td>\n      <td>-1.804653</td>\n      <td>0.209782</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1023</th>\n      <td>-1.161344</td>\n      <td>2.041093</td>\n      <td>-0.846733</td>\n      <td>-0.616613</td>\n      <td>1.003599</td>\n      <td>0.982343</td>\n      <td>-1.064914</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1024</th>\n      <td>-1.102018</td>\n      <td>2.153519</td>\n      <td>-0.846733</td>\n      <td>0.076459</td>\n      <td>1.087297</td>\n      <td>-1.467693</td>\n      <td>0.663784</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1025</th>\n      <td>-0.045623</td>\n      <td>0.487998</td>\n      <td>0.564271</td>\n      <td>-0.092126</td>\n      <td>0.451190</td>\n      <td>-1.322363</td>\n      <td>-0.065861</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1026</th>\n      <td>0.392628</td>\n      <td>-0.856472</td>\n      <td>0.959602</td>\n      <td>0.675872</td>\n      <td>0.702285</td>\n      <td>-1.993711</td>\n      <td>0.496651</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1027</th>\n      <td>-1.269472</td>\n      <td>0.759210</td>\n      <td>0.850222</td>\n      <td>0.521336</td>\n      <td>-0.017520</td>\n      <td>-1.035561</td>\n      <td>0.080068</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1028</th>\n      <td>-1.168042</td>\n      <td>1.307430</td>\n      <td>-0.846733</td>\n      <td>-0.279443</td>\n      <td>0.852942</td>\n      <td>0.214537</td>\n      <td>0.191074</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1029</th>\n      <td>-0.193939</td>\n      <td>0.308349</td>\n      <td>0.376762</td>\n      <td>0.891286</td>\n      <td>0.400971</td>\n      <td>-1.394385</td>\n      <td>-0.150675</td>\n      <td>-0.279597</td>\n    </tr>\n  </tbody>\n</table>\n<p>1030 rows \u00d7 8 columns</p>\n</div>",
                        "text/plain": "        Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n0     2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n1     2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n2     0.491187            0.795140 -0.846733  2.174405         -1.038638   \n3     0.491187            0.795140 -0.846733  2.174405         -1.038638   \n4    -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n5    -0.145138            0.464818 -0.846733  2.174405         -1.038638   \n6     0.945704            0.244603 -0.846733  2.174405         -1.038638   \n7     0.945704            0.244603 -0.846733  2.174405         -1.038638   \n8    -0.145138            0.464818 -0.846733  2.174405         -1.038638   \n9     1.854740           -0.856472 -0.846733  2.174405         -1.038638   \n10   -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n11   -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n12    1.400222           -0.305934 -0.846733  2.174405         -1.038638   \n13   -0.872367            1.345678 -0.846733  2.174405         -1.038638   \n14    0.218476            0.024388 -0.846733  2.174405         -1.038638   \n15    0.945704           -0.856472 -0.846733  2.174405         -1.038638   \n16   -1.354634            1.570529 -0.846733  0.488555         -1.038638   \n17    0.582090           -0.416042 -0.846733  2.174405         -1.038638   \n18    0.945704            0.244603 -0.846733  2.174405         -1.038638   \n19    1.854740           -0.856472 -0.846733  2.174405         -1.038638   \n20    1.400222           -0.305934 -0.846733  2.174405         -1.038638   \n21   -1.354634            1.570529 -0.846733  0.488555         -1.038638   \n22   -1.354634            1.570529 -0.846733  0.488555         -1.038638   \n23   -1.354634            1.570529 -0.846733  0.488555         -1.038638   \n24    0.945704           -0.856472 -0.846733  2.174405         -1.038638   \n25    0.945704           -0.856472 -0.846733  2.174405         -1.038638   \n26    0.945704            0.244603 -0.846733  2.174405         -1.038638   \n27    0.582090           -0.416042 -0.846733  2.174405         -1.038638   \n28    1.400222           -0.305934 -0.846733  2.174405         -1.038638   \n29    1.854740           -0.856472 -0.846733  2.174405         -1.038638   \n...        ...                 ...       ...       ...               ...   \n1000 -1.332626            1.074465  1.179925 -0.377784          0.785983   \n1001  0.159150            0.733712  0.823658  0.924067         -0.034259   \n1002  0.384016            1.046649 -0.846733  0.418312         -0.268614   \n1003  0.811741            1.310907 -0.846733  0.455775          0.066178   \n1004 -0.013089            0.637513  0.722091 -0.429296          0.551628   \n1005 -0.278144            0.268942  0.334573  0.572848          0.350753   \n1006 -1.115414           -0.856472  1.503377  0.001532          0.919901   \n1007 -1.201533            1.965756 -0.846733 -0.059346          0.752504   \n1008 -1.157517            1.322497  1.440874  1.013042          0.852942   \n1009  0.162020           -0.856472  0.825221  0.226312         -0.017520   \n1010  0.351482           -0.856472  1.129922  1.317431         -0.084478   \n1011  0.058677            0.540155  0.620524  0.282507          0.501409   \n1012  0.425162            1.072147 -0.846733 -0.354369          0.451190   \n1013  0.715097           -0.856472  1.365871  0.549433          0.802723   \n1014 -1.427357            1.536917  1.667448 -0.124907         -0.117958   \n1015  0.395499            0.865841 -0.846733  0.198215          0.384232   \n1016 -1.119242           -0.856472  2.279976 -0.017199          1.070557   \n1017  0.312250           -0.856472  0.912725 -0.546369          0.652066   \n1018  0.384973           -0.856472  1.151798  0.043678          0.886421   \n1019 -1.353677            1.043172  1.148673  2.581819         -0.067739   \n1020  0.069203            0.545950 -0.846733 -0.195150          0.133137   \n1021  0.162977           -0.856472  0.825221  1.317431          0.819463   \n1022 -0.159491            0.430047  0.504893  0.652457         -0.050999   \n1023 -1.161344            2.041093 -0.846733 -0.616613          1.003599   \n1024 -1.102018            2.153519 -0.846733  0.076459          1.087297   \n1025 -0.045623            0.487998  0.564271 -0.092126          0.451190   \n1026  0.392628           -0.856472  0.959602  0.675872          0.702285   \n1027 -1.269472            0.759210  0.850222  0.521336         -0.017520   \n1028 -1.168042            1.307430 -0.846733 -0.279443          0.852942   \n1029 -0.193939            0.308349  0.376762  0.891286          0.400971   \n\n      Coarse Aggregate  Fine Aggregate       Age  \n0             0.862735       -1.217079 -0.279597  \n1             1.055651       -1.217079 -0.279597  \n2            -0.526262       -2.239829  3.551340  \n3            -0.526262       -2.239829  5.055221  \n4             0.070492        0.647569  4.976069  \n5            -0.526262       -1.291914  0.701883  \n6            -0.526262       -2.239829  5.055221  \n7            -0.526262       -2.239829 -0.279597  \n8            -0.526262       -1.291914 -0.279597  \n9            -0.526262       -2.239829 -0.279597  \n10            0.070492        0.647569  0.701883  \n11            0.070492        0.647569 -0.279597  \n12           -0.526262       -2.239829  3.551340  \n13           -0.526262       -1.291914  0.701883  \n14           -0.526262       -1.291914 -0.279597  \n15           -0.526262       -1.291914  0.701883  \n16            0.952763        0.415580  0.701883  \n17           -0.526262       -1.291914  5.055221  \n18           -0.526262       -2.239829  0.701883  \n19           -0.526262       -2.239829  2.126612  \n20           -0.526262       -2.239829  2.126612  \n21            0.952763        0.415580 -0.279597  \n22            0.952763        0.415580 -0.675355  \n23            0.952763        0.415580  2.126612  \n24           -0.526262       -1.291914  5.055221  \n25           -0.526262       -1.291914  3.551340  \n26           -0.526262       -2.239829  3.551340  \n27           -0.526262       -1.291914  2.126612  \n28           -0.526262       -2.239829 -0.279597  \n29           -0.526262       -2.239829 -0.612034  \n...                ...             ...       ...  \n1000         -1.161599        0.146172 -0.279597  \n1001         -1.215616       -1.475261 -0.279597  \n1002         -1.323649        0.005232 -0.279597  \n1003         -1.911400       -0.208048 -0.279597  \n1004         -1.901111        0.390635 -0.279597  \n1005         -1.767356        0.596432 -0.279597  \n1006          0.647955       -0.557280 -0.279597  \n1007          0.631236       -0.946424 -0.279597  \n1008         -1.854812       -0.796753 -0.279597  \n1009         -1.207899        0.519102 -0.279597  \n1010         -1.445829       -0.461241 -0.279597  \n1011         -0.881228       -0.968875 -0.279597  \n1012         -1.174460        0.204793 -0.279597  \n1013         -2.205919        0.060112 -0.279597  \n1014         -1.363518       -0.473714 -0.279597  \n1015         -0.281901       -0.799248 -0.279597  \n1016         -1.589873        0.903257 -0.279597  \n1017         -0.612431        0.116238 -0.279597  \n1018         -1.322363        0.076326 -0.279597  \n1019         -1.341654       -1.471519 -0.279597  \n1020         -0.836214        0.697460 -0.279597  \n1021         -1.200182       -0.366450 -0.279597  \n1022         -1.804653        0.209782 -0.279597  \n1023          0.982343       -1.064914 -0.279597  \n1024         -1.467693        0.663784 -0.279597  \n1025         -1.322363       -0.065861 -0.279597  \n1026         -1.993711        0.496651 -0.279597  \n1027         -1.035561        0.080068 -0.279597  \n1028          0.214537        0.191074 -0.279597  \n1029         -1.394385       -0.150675 -0.279597  \n\n[1030 rows x 8 columns]"
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "predictors_norm"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "*You'll notice that as the model iterates through the dataset, the accuracy is best at the 20th epoch"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "*Run the same model with 100 epochs rather than 50"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/100\n721/721 [==============================] - 1s 694us/step - loss: 124.1890 - val_loss: 127.7722\nEpoch 2/100\n721/721 [==============================] - 1s 696us/step - loss: 123.6325 - val_loss: 128.0912\nEpoch 3/100\n721/721 [==============================] - 0s 690us/step - loss: 122.8956 - val_loss: 127.8010\nEpoch 4/100\n721/721 [==============================] - 0s 693us/step - loss: 122.2486 - val_loss: 126.3087\nEpoch 5/100\n721/721 [==============================] - 0s 691us/step - loss: 121.5028 - val_loss: 125.7660\nEpoch 6/100\n721/721 [==============================] - 0s 692us/step - loss: 121.1849 - val_loss: 125.6781\nEpoch 7/100\n721/721 [==============================] - 1s 719us/step - loss: 121.1907 - val_loss: 125.1315\nEpoch 8/100\n721/721 [==============================] - 0s 692us/step - loss: 119.6944 - val_loss: 126.2065\nEpoch 9/100\n721/721 [==============================] - 1s 1ms/step - loss: 119.0874 - val_loss: 124.9528\nEpoch 10/100\n721/721 [==============================] - 1s 2ms/step - loss: 118.5324 - val_loss: 124.4182\nEpoch 11/100\n721/721 [==============================] - 1s 698us/step - loss: 117.5841 - val_loss: 124.3577\nEpoch 12/100\n721/721 [==============================] - 1s 714us/step - loss: 116.9371 - val_loss: 124.0725\nEpoch 13/100\n721/721 [==============================] - 0s 691us/step - loss: 116.1801 - val_loss: 122.3839\nEpoch 14/100\n721/721 [==============================] - 1s 695us/step - loss: 115.3331 - val_loss: 122.6499\nEpoch 15/100\n721/721 [==============================] - 1s 695us/step - loss: 114.7106 - val_loss: 120.5837\nEpoch 16/100\n721/721 [==============================] - 1s 702us/step - loss: 114.1281 - val_loss: 122.2777\nEpoch 17/100\n721/721 [==============================] - 1s 711us/step - loss: 113.1943 - val_loss: 121.1193\nEpoch 18/100\n721/721 [==============================] - 1s 719us/step - loss: 112.5726 - val_loss: 120.8671\nEpoch 19/100\n721/721 [==============================] - 0s 693us/step - loss: 111.7651 - val_loss: 119.8239\nEpoch 20/100\n721/721 [==============================] - 1s 721us/step - loss: 111.0998 - val_loss: 119.9427\nEpoch 21/100\n721/721 [==============================] - 0s 690us/step - loss: 110.8217 - val_loss: 119.7706\nEpoch 22/100\n721/721 [==============================] - 1s 721us/step - loss: 109.7705 - val_loss: 119.2419\nEpoch 23/100\n721/721 [==============================] - 1s 718us/step - loss: 108.7993 - val_loss: 118.9469\nEpoch 24/100\n721/721 [==============================] - 1s 696us/step - loss: 108.2396 - val_loss: 117.7821\nEpoch 25/100\n721/721 [==============================] - 0s 668us/step - loss: 107.4454 - val_loss: 117.5888\nEpoch 26/100\n721/721 [==============================] - 0s 664us/step - loss: 106.6876 - val_loss: 117.2885\nEpoch 27/100\n721/721 [==============================] - 0s 692us/step - loss: 106.1142 - val_loss: 116.7937\nEpoch 28/100\n721/721 [==============================] - 1s 695us/step - loss: 105.3022 - val_loss: 115.5479\nEpoch 29/100\n721/721 [==============================] - 1s 1ms/step - loss: 104.7119 - val_loss: 115.1810\nEpoch 30/100\n721/721 [==============================] - 1s 1ms/step - loss: 103.7220 - val_loss: 115.0309\nEpoch 31/100\n721/721 [==============================] - 1s 695us/step - loss: 102.9815 - val_loss: 114.5257\nEpoch 32/100\n721/721 [==============================] - 1s 720us/step - loss: 102.2349 - val_loss: 114.1604\nEpoch 33/100\n721/721 [==============================] - 0s 690us/step - loss: 101.6107 - val_loss: 114.2384\nEpoch 34/100\n721/721 [==============================] - 1s 695us/step - loss: 100.7676 - val_loss: 113.9218\nEpoch 35/100\n721/721 [==============================] - 1s 719us/step - loss: 99.8721 - val_loss: 112.5027\nEpoch 36/100\n721/721 [==============================] - 1s 702us/step - loss: 99.2163 - val_loss: 112.0763\nEpoch 37/100\n721/721 [==============================] - 0s 654us/step - loss: 98.7315 - val_loss: 110.0292\nEpoch 38/100\n721/721 [==============================] - 0s 692us/step - loss: 97.6694 - val_loss: 110.7312\nEpoch 39/100\n721/721 [==============================] - 1s 1ms/step - loss: 96.4557 - val_loss: 109.6161\nEpoch 40/100\n721/721 [==============================] - 1s 695us/step - loss: 95.5122 - val_loss: 110.5485\nEpoch 41/100\n721/721 [==============================] - 1s 745us/step - loss: 94.7425 - val_loss: 109.1111\nEpoch 42/100\n721/721 [==============================] - 1s 751us/step - loss: 93.9257 - val_loss: 108.3984\nEpoch 43/100\n721/721 [==============================] - 1s 694us/step - loss: 92.6808 - val_loss: 108.2385\nEpoch 44/100\n721/721 [==============================] - 1s 694us/step - loss: 91.9773 - val_loss: 107.0943\nEpoch 45/100\n721/721 [==============================] - 1s 694us/step - loss: 91.2342 - val_loss: 105.6322\nEpoch 46/100\n721/721 [==============================] - 1s 696us/step - loss: 90.3059 - val_loss: 105.5264\nEpoch 47/100\n721/721 [==============================] - 0s 693us/step - loss: 89.4544 - val_loss: 105.2480\nEpoch 48/100\n721/721 [==============================] - 0s 692us/step - loss: 88.7494 - val_loss: 104.8726\nEpoch 49/100\n721/721 [==============================] - 1s 2ms/step - loss: 87.8302 - val_loss: 103.7711\nEpoch 50/100\n721/721 [==============================] - 0s 693us/step - loss: 87.1799 - val_loss: 103.1588\nEpoch 51/100\n721/721 [==============================] - 1s 698us/step - loss: 86.4529 - val_loss: 101.2777\nEpoch 52/100\n721/721 [==============================] - 1s 712us/step - loss: 85.6583 - val_loss: 101.6032\nEpoch 53/100\n721/721 [==============================] - 1s 720us/step - loss: 84.9589 - val_loss: 101.3868\nEpoch 54/100\n721/721 [==============================] - 0s 693us/step - loss: 84.0086 - val_loss: 100.6303\nEpoch 55/100\n721/721 [==============================] - 1s 719us/step - loss: 83.5055 - val_loss: 99.9336\nEpoch 56/100\n721/721 [==============================] - 1s 694us/step - loss: 82.6475 - val_loss: 98.3958\nEpoch 57/100\n721/721 [==============================] - 1s 1ms/step - loss: 81.9796 - val_loss: 98.0396\nEpoch 58/100\n721/721 [==============================] - 1s 719us/step - loss: 81.1649 - val_loss: 97.0238\nEpoch 59/100\n721/721 [==============================] - 1s 775us/step - loss: 80.4771 - val_loss: 96.9002\nEpoch 60/100\n721/721 [==============================] - 0s 691us/step - loss: 79.8673 - val_loss: 97.3018\nEpoch 61/100\n721/721 [==============================] - 0s 690us/step - loss: 79.0264 - val_loss: 96.2008\nEpoch 62/100\n721/721 [==============================] - 1s 724us/step - loss: 78.4164 - val_loss: 95.1536\nEpoch 63/100\n721/721 [==============================] - 1s 772us/step - loss: 77.7959 - val_loss: 94.3070\nEpoch 64/100\n721/721 [==============================] - 0s 692us/step - loss: 77.1160 - val_loss: 93.8870\nEpoch 65/100\n721/721 [==============================] - 0s 672us/step - loss: 76.5062 - val_loss: 94.1343\nEpoch 66/100\n721/721 [==============================] - 1s 701us/step - loss: 75.7390 - val_loss: 93.4506\nEpoch 67/100\n721/721 [==============================] - 1s 711us/step - loss: 75.2260 - val_loss: 91.6892\nEpoch 68/100\n721/721 [==============================] - 1s 1ms/step - loss: 74.5978 - val_loss: 92.8915\nEpoch 69/100\n721/721 [==============================] - 1s 1ms/step - loss: 74.1225 - val_loss: 90.9571\nEpoch 70/100\n721/721 [==============================] - 0s 668us/step - loss: 73.1537 - val_loss: 90.5847\nEpoch 71/100\n721/721 [==============================] - 1s 718us/step - loss: 72.6500 - val_loss: 90.5190\nEpoch 72/100\n721/721 [==============================] - 1s 694us/step - loss: 71.9078 - val_loss: 90.2228\nEpoch 73/100\n721/721 [==============================] - 0s 692us/step - loss: 71.3073 - val_loss: 89.3371\nEpoch 74/100\n721/721 [==============================] - 1s 696us/step - loss: 70.8774 - val_loss: 89.0238\nEpoch 75/100\n721/721 [==============================] - 0s 661us/step - loss: 69.8638 - val_loss: 88.6774\nEpoch 76/100\n721/721 [==============================] - 1s 699us/step - loss: 69.6075 - val_loss: 88.0172\nEpoch 77/100\n721/721 [==============================] - 1s 712us/step - loss: 68.8530 - val_loss: 88.0062\nEpoch 78/100\n721/721 [==============================] - 0s 691us/step - loss: 68.0551 - val_loss: 86.6650\nEpoch 79/100\n721/721 [==============================] - 0s 668us/step - loss: 67.7091 - val_loss: 87.0806\nEpoch 80/100\n721/721 [==============================] - 1s 694us/step - loss: 66.8632 - val_loss: 85.0721\nEpoch 81/100\n721/721 [==============================] - 1s 715us/step - loss: 66.3032 - val_loss: 85.3381\nEpoch 82/100\n721/721 [==============================] - 0s 665us/step - loss: 65.6779 - val_loss: 85.8837\nEpoch 83/100\n721/721 [==============================] - 0s 693us/step - loss: 65.3001 - val_loss: 83.6292\nEpoch 84/100\n721/721 [==============================] - 1s 720us/step - loss: 64.6613 - val_loss: 83.9659\nEpoch 85/100\n721/721 [==============================] - 1s 720us/step - loss: 63.8199 - val_loss: 83.6733\nEpoch 86/100\n721/721 [==============================] - 1s 750us/step - loss: 63.3600 - val_loss: 83.7199\nEpoch 87/100\n721/721 [==============================] - 1s 1ms/step - loss: 62.4714 - val_loss: 82.4827\nEpoch 88/100\n721/721 [==============================] - 1s 1ms/step - loss: 61.9861 - val_loss: 81.4302\nEpoch 89/100\n721/721 [==============================] - 1s 1ms/step - loss: 61.3117 - val_loss: 81.5432\nEpoch 90/100\n721/721 [==============================] - 1s 754us/step - loss: 60.8120 - val_loss: 81.4460\nEpoch 91/100\n721/721 [==============================] - 0s 690us/step - loss: 60.2786 - val_loss: 80.2791\nEpoch 92/100\n721/721 [==============================] - 0s 685us/step - loss: 59.8366 - val_loss: 79.7229\nEpoch 93/100\n721/721 [==============================] - 1s 699us/step - loss: 59.4292 - val_loss: 78.8287\nEpoch 94/100\n721/721 [==============================] - 1s 712us/step - loss: 58.8916 - val_loss: 80.2364\nEpoch 95/100\n721/721 [==============================] - 0s 693us/step - loss: 58.3355 - val_loss: 79.3884\nEpoch 96/100\n721/721 [==============================] - 1s 695us/step - loss: 57.6855 - val_loss: 77.8019\nEpoch 97/100\n721/721 [==============================] - 1s 720us/step - loss: 57.3948 - val_loss: 76.7418\nEpoch 98/100\n721/721 [==============================] - 1s 754us/step - loss: 56.9872 - val_loss: 78.8770\nEpoch 99/100\n721/721 [==============================] - 0s 659us/step - loss: 56.4264 - val_loss: 76.8849\nEpoch 100/100\n721/721 [==============================] - 0s 669us/step - loss: 56.0625 - val_loss: 77.2022\n"
                },
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f62300e06a0>"
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model version C\nmodel.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=1)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "*Best accuracy is achieved at epoch 97...Overall accuracy has improved with added epochs"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": "# newly defined regression model with additional hidden layers\ndef regression_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train on 721 samples, validate on 309 samples\nEpoch 1/100\n721/721 [==============================] - 0s 669us/step - loss: 55.5132 - val_loss: 76.2413\nEpoch 2/100\n721/721 [==============================] - 0s 693us/step - loss: 55.1546 - val_loss: 75.4279\nEpoch 3/100\n721/721 [==============================] - 1s 694us/step - loss: 54.9765 - val_loss: 75.6808\nEpoch 4/100\n721/721 [==============================] - 0s 690us/step - loss: 54.3321 - val_loss: 74.6151\nEpoch 5/100\n721/721 [==============================] - 0s 663us/step - loss: 53.9666 - val_loss: 75.6414\nEpoch 6/100\n721/721 [==============================] - 1s 694us/step - loss: 53.6699 - val_loss: 73.5960\nEpoch 7/100\n721/721 [==============================] - 0s 693us/step - loss: 53.2280 - val_loss: 73.5329\nEpoch 8/100\n721/721 [==============================] - 0s 689us/step - loss: 52.9745 - val_loss: 73.0802\nEpoch 9/100\n721/721 [==============================] - 1s 706us/step - loss: 52.6997 - val_loss: 73.5702\nEpoch 10/100\n721/721 [==============================] - 1s 1ms/step - loss: 52.2136 - val_loss: 72.1851\nEpoch 11/100\n721/721 [==============================] - 1s 698us/step - loss: 52.0124 - val_loss: 72.4868\nEpoch 12/100\n721/721 [==============================] - 0s 688us/step - loss: 51.8531 - val_loss: 71.1322\nEpoch 13/100\n721/721 [==============================] - 0s 692us/step - loss: 51.5155 - val_loss: 72.7956\nEpoch 14/100\n721/721 [==============================] - 0s 667us/step - loss: 51.0582 - val_loss: 70.8246\nEpoch 15/100\n721/721 [==============================] - 0s 674us/step - loss: 50.8076 - val_loss: 70.4206\nEpoch 16/100\n721/721 [==============================] - 0s 653us/step - loss: 50.4407 - val_loss: 71.1178\nEpoch 17/100\n721/721 [==============================] - 1s 698us/step - loss: 50.3810 - val_loss: 70.2937\nEpoch 18/100\n721/721 [==============================] - 0s 687us/step - loss: 49.9818 - val_loss: 69.6271\nEpoch 19/100\n721/721 [==============================] - 1s 694us/step - loss: 49.6859 - val_loss: 69.0271\nEpoch 20/100\n721/721 [==============================] - 1s 2ms/step - loss: 49.4361 - val_loss: 69.7306\nEpoch 21/100\n721/721 [==============================] - 1s 705us/step - loss: 49.3565 - val_loss: 68.4044\nEpoch 22/100\n721/721 [==============================] - 1s 719us/step - loss: 49.1238 - val_loss: 68.0169\nEpoch 23/100\n721/721 [==============================] - 0s 691us/step - loss: 48.7407 - val_loss: 68.7204\nEpoch 24/100\n721/721 [==============================] - 0s 664us/step - loss: 48.6286 - val_loss: 67.1900\nEpoch 25/100\n721/721 [==============================] - 0s 691us/step - loss: 48.4837 - val_loss: 68.3218\nEpoch 26/100\n721/721 [==============================] - 0s 693us/step - loss: 48.2589 - val_loss: 66.9900\nEpoch 27/100\n721/721 [==============================] - 0s 692us/step - loss: 48.2701 - val_loss: 67.6222\nEpoch 28/100\n721/721 [==============================] - 0s 692us/step - loss: 47.6392 - val_loss: 67.7304\nEpoch 29/100\n721/721 [==============================] - 1s 694us/step - loss: 47.6963 - val_loss: 64.8098\nEpoch 30/100\n721/721 [==============================] - 1s 700us/step - loss: 47.1932 - val_loss: 67.6553\nEpoch 31/100\n721/721 [==============================] - 1s 697us/step - loss: 47.2292 - val_loss: 68.2421\nEpoch 32/100\n721/721 [==============================] - 1s 717us/step - loss: 46.9575 - val_loss: 65.7559\nEpoch 33/100\n721/721 [==============================] - 1s 720us/step - loss: 46.8565 - val_loss: 66.1253\nEpoch 34/100\n721/721 [==============================] - 1s 694us/step - loss: 46.5622 - val_loss: 65.9783\nEpoch 35/100\n721/721 [==============================] - 1s 719us/step - loss: 46.3486 - val_loss: 65.7533\nEpoch 36/100\n721/721 [==============================] - 0s 691us/step - loss: 46.2080 - val_loss: 66.0361\nEpoch 37/100\n721/721 [==============================] - 1s 694us/step - loss: 46.0822 - val_loss: 66.6573\nEpoch 38/100\n721/721 [==============================] - 1s 719us/step - loss: 45.9744 - val_loss: 65.0695\nEpoch 39/100\n721/721 [==============================] - 0s 688us/step - loss: 45.6580 - val_loss: 65.3247\nEpoch 40/100\n721/721 [==============================] - 1s 1ms/step - loss: 45.6295 - val_loss: 66.2611\nEpoch 41/100\n721/721 [==============================] - 1s 1ms/step - loss: 45.4200 - val_loss: 66.1671\nEpoch 42/100\n721/721 [==============================] - 1s 719us/step - loss: 45.2207 - val_loss: 63.8222\nEpoch 43/100\n721/721 [==============================] - 0s 692us/step - loss: 45.0377 - val_loss: 65.5851\nEpoch 44/100\n721/721 [==============================] - 1s 699us/step - loss: 44.9210 - val_loss: 63.3070\nEpoch 45/100\n721/721 [==============================] - 0s 687us/step - loss: 44.7201 - val_loss: 65.8417\nEpoch 46/100\n721/721 [==============================] - 1s 1ms/step - loss: 44.7953 - val_loss: 64.1680\nEpoch 47/100\n721/721 [==============================] - 0s 692us/step - loss: 44.6500 - val_loss: 64.7082\nEpoch 48/100\n721/721 [==============================] - 0s 669us/step - loss: 44.3385 - val_loss: 65.8146\nEpoch 49/100\n721/721 [==============================] - 0s 687us/step - loss: 44.1302 - val_loss: 63.1344\nEpoch 50/100\n721/721 [==============================] - 0s 638us/step - loss: 43.9507 - val_loss: 65.6578\nEpoch 51/100\n721/721 [==============================] - 0s 693us/step - loss: 43.9525 - val_loss: 63.6888\nEpoch 52/100\n721/721 [==============================] - 0s 689us/step - loss: 43.7930 - val_loss: 64.1350\nEpoch 53/100\n721/721 [==============================] - 0s 667us/step - loss: 43.4295 - val_loss: 64.2485\nEpoch 54/100\n721/721 [==============================] - 0s 692us/step - loss: 43.4170 - val_loss: 64.3652\nEpoch 55/100\n721/721 [==============================] - 0s 691us/step - loss: 43.2726 - val_loss: 64.2830\nEpoch 56/100\n721/721 [==============================] - 0s 693us/step - loss: 43.0937 - val_loss: 64.1143\nEpoch 57/100\n721/721 [==============================] - 0s 691us/step - loss: 42.8520 - val_loss: 63.9933\nEpoch 58/100\n721/721 [==============================] - 0s 667us/step - loss: 42.8265 - val_loss: 64.9679\nEpoch 59/100\n721/721 [==============================] - 0s 667us/step - loss: 42.6302 - val_loss: 65.3414\nEpoch 60/100\n721/721 [==============================] - 1s 1ms/step - loss: 42.4993 - val_loss: 63.6852\nEpoch 61/100\n721/721 [==============================] - 1s 1ms/step - loss: 42.2360 - val_loss: 63.2685\nEpoch 62/100\n721/721 [==============================] - 1s 695us/step - loss: 42.1623 - val_loss: 63.6842\nEpoch 63/100\n721/721 [==============================] - 1s 713us/step - loss: 41.9271 - val_loss: 63.8142\nEpoch 64/100\n721/721 [==============================] - 0s 690us/step - loss: 41.8203 - val_loss: 65.1589\nEpoch 65/100\n721/721 [==============================] - 0s 668us/step - loss: 41.8357 - val_loss: 64.3287\nEpoch 66/100\n721/721 [==============================] - 0s 664us/step - loss: 41.6343 - val_loss: 64.4029\nEpoch 67/100\n721/721 [==============================] - 1s 751us/step - loss: 41.3583 - val_loss: 63.8815\nEpoch 68/100\n721/721 [==============================] - 1s 715us/step - loss: 41.2430 - val_loss: 63.2861\nEpoch 69/100\n721/721 [==============================] - 1s 695us/step - loss: 41.0902 - val_loss: 63.5869\nEpoch 70/100\n721/721 [==============================] - 1s 716us/step - loss: 40.9768 - val_loss: 63.2747\nEpoch 71/100\n721/721 [==============================] - 1s 721us/step - loss: 40.9474 - val_loss: 64.1707\nEpoch 72/100\n721/721 [==============================] - 0s 672us/step - loss: 40.8560 - val_loss: 63.5150\nEpoch 73/100\n721/721 [==============================] - 0s 693us/step - loss: 40.6538 - val_loss: 64.6535\nEpoch 74/100\n721/721 [==============================] - 1s 715us/step - loss: 40.7750 - val_loss: 65.8602\nEpoch 75/100\n721/721 [==============================] - 1s 1ms/step - loss: 40.6436 - val_loss: 63.5337\nEpoch 76/100\n721/721 [==============================] - 0s 691us/step - loss: 40.4023 - val_loss: 65.4214\nEpoch 77/100\n721/721 [==============================] - 0s 693us/step - loss: 40.2112 - val_loss: 63.4259\nEpoch 78/100\n721/721 [==============================] - 1s 695us/step - loss: 40.2549 - val_loss: 63.4848\nEpoch 79/100\n721/721 [==============================] - 1s 711us/step - loss: 39.9865 - val_loss: 64.1079\nEpoch 80/100\n721/721 [==============================] - 1s 2ms/step - loss: 39.8217 - val_loss: 63.3426\nEpoch 81/100\n721/721 [==============================] - 1s 721us/step - loss: 39.7385 - val_loss: 64.9680\nEpoch 82/100\n721/721 [==============================] - 1s 721us/step - loss: 39.6729 - val_loss: 63.4064\nEpoch 83/100\n721/721 [==============================] - 0s 692us/step - loss: 39.5795 - val_loss: 63.7221\nEpoch 84/100\n721/721 [==============================] - 0s 693us/step - loss: 39.4481 - val_loss: 64.0998\nEpoch 85/100\n721/721 [==============================] - 0s 693us/step - loss: 39.1456 - val_loss: 65.6276\nEpoch 86/100\n721/721 [==============================] - 0s 669us/step - loss: 39.2934 - val_loss: 64.8866\nEpoch 87/100\n721/721 [==============================] - 0s 691us/step - loss: 39.1702 - val_loss: 65.7566\nEpoch 88/100\n721/721 [==============================] - 1s 698us/step - loss: 38.9110 - val_loss: 64.5835\nEpoch 89/100\n721/721 [==============================] - 1s 717us/step - loss: 38.8519 - val_loss: 65.0532\nEpoch 90/100\n721/721 [==============================] - 0s 667us/step - loss: 38.8902 - val_loss: 66.3648\nEpoch 91/100\n721/721 [==============================] - 0s 638us/step - loss: 38.7043 - val_loss: 64.3484\nEpoch 92/100\n721/721 [==============================] - 0s 638us/step - loss: 39.2752 - val_loss: 65.1215\nEpoch 93/100\n721/721 [==============================] - 1s 696us/step - loss: 38.7051 - val_loss: 66.9176\nEpoch 94/100\n721/721 [==============================] - 1s 694us/step - loss: 38.4331 - val_loss: 65.1463\nEpoch 95/100\n721/721 [==============================] - 0s 692us/step - loss: 38.3544 - val_loss: 64.8343\nEpoch 96/100\n721/721 [==============================] - 1s 702us/step - loss: 38.2559 - val_loss: 65.3591\nEpoch 97/100\n721/721 [==============================] - 1s 694us/step - loss: 38.1107 - val_loss: 66.3745\nEpoch 98/100\n721/721 [==============================] - 1s 700us/step - loss: 38.1647 - val_loss: 65.4834\nEpoch 99/100\n721/721 [==============================] - 0s 668us/step - loss: 38.0300 - val_loss: 65.1894\nEpoch 100/100\n721/721 [==============================] - 1s 1ms/step - loss: 37.8840 - val_loss: 66.0640\n"
                },
                {
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f62300e05f8>"
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model fit with additional layers\nmodel.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=1)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "*Best accuracy is acheived at epoch 82...Overall accuracy has improved when adding an additional hidden layer"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}